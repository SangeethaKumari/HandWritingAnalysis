Readme document for AI Knowledge Graph Builder
Automatically extract concepts, relationships, and insights from any AI/ML PDF using an LLM

This project builds a full AI Knowledge Graph from a PDF document using:

Ollama LLM (llama3.1 or any supported model)

NetworkX for building the graph

Matplotlib for visualization

PyPDF2 for text extraction

WordCloud for importance-based cloud

Semantic reasoning for inferring new relationships

The output includes:

âœ” Extracted AI concepts and relationships (triplets)
âœ” A directed knowledge graph
âœ” Centrality scores (degree, betweenness, eigenvector, PageRank, etc.)
âœ” Semantic inference
âœ” Graph visualization PNG
âœ” Word cloud PNG
âœ” Exported JSON files

ğŸ”¥ Features
1. PDF â†’ Text Extraction

Extracts all text from a PDF using PyPDF2 and splits the content by pages.

2. LLM-Powered Triplet Extraction

Uses an Ollama model to extract knowledge graph triplets such as:

["Neural Network", "uses", "Backpropagation"]
["Gradient Descent", "optimizes", "Loss Function"]

3. Knowledge Graph Construction

Builds a directed graph (DiGraph) using NetworkX.

4. Graph Analytics

Calculates 7 centrality measures:

Degree

In-degree

Out-degree

Betweenness

Closeness

Eigenvector

PageRank

5. Semantic Reasoning

Automatically infers new relationships:

If A â†’ B and B â†’ C, the system infers A â†’ C.

6. Visual Output

Generates:

knowledge_graph.png

wordcloud.png

7. JSON Export

Creates:

triplets.json

centrality_measures.json

ğŸ§© Project Structure
AIKnowledgeGraph/
â”‚
â”œâ”€â”€ ai_knowledge_graph.py    # Main code (your file)
â”œâ”€â”€ README.md                # Documentation
â”œâ”€â”€ sample.pdf               # Your input PDF
â”‚
â”œâ”€â”€ triplets.json            # Exported triplets
â”œâ”€â”€ centrality_measures.json # Exported centrality
â”œâ”€â”€ knowledge_graph.png      # Graph visualization
â””â”€â”€ wordcloud.png            # Word cloud

ğŸš€ How It Works (Pipeline Overview)
Step 1 â€” Extract PDF Text
text = kg.extract_text_from_pdf(pdf_path)


Uses PyPDF2 to extract and mark pages.

Step 2 â€” Chunk Text

Large PDFs are split into manageable chunks for LLM processing:

kg.chunk_markdown(text, chunk_size=3000)

Step 3 â€” Extract Triplets (LLM)

Ollama model is used to extract 10â€“15 triplets per chunk:

response = self.client.chat(...)

Step 4 â€” Build the Knowledge Graph

Adds each extracted relationship as:

subject --predicate--> object

Step 5 â€” Compute Graph Analytics

NetworkX is used to compute centrality scores.

Step 6 â€” Semantic Inference

Automatic reasoning:
If Aâ†’B and Bâ†’C, infer Aâ†’C.

Step 7 â€” Visualize

Creates:

Graph visualization (top PageRank nodes)

Word cloud (importance-based)

Step 8 â€” Export

Triplets and metrics exported as JSON.

ğŸ’» Usage
Run the script
python ai_knowledge_graph.py

Or call main() manually
kg = main("lecture_notes.pdf", model="llama3.1")

ğŸ§ª Requirements

Install dependencies:

pip install networkx matplotlib wordcloud ollama PyPDF2


Make sure Ollama is installed and running:

ollama pull llama3.1

ğŸ§  Example Output (Triplet)
1. (Neural Network) --[uses]--> (Activation Function)
2. (Transformer) --[based_on]--> (Self-Attention)
3. (Loss Function) --[optimized_by]--> (Gradient Descent)

ğŸ–¼ Example Visualization

Size = PageRank importance

Color = Betweenness score

Edges show direction + relationships

ğŸ“‚ Exports
triplets.json

Structured list of all extracted relationships.

centrality_measures.json

Full analytics for every node.

ğŸ Final Notes

This system is ideal for:

AI/ML learning material

Summaries of textbooks

Lecture notes

Research papers

Technical documentation
